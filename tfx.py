# -*- coding: utf-8 -*-
"""tfx.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BxewM7lEpc8hiP3pdlbpmIpyprHdSvRf

## **TensorFlow Extended (TFX)**
Is an end-to-end platform for deploying production ML pipelines

"""

!pip install -U tfx

import os

# Path to the directory containing CSV files
directory_path = '/content/drive/MyDrive/1_movies_per_genre'

# List all files in the directory
files = os.listdir(directory_path)
print("Files in directory:", files)

# Step 3: Read All CSV Files
import pandas as pd

# Initialize an empty dictionary to hold DataFrames
dataframes = {}

# Iterate over each file in the directory
for file in files:
    # Check if the file is a CSV
    if file.endswith('.csv'):
        # Construct full file path
        file_path = os.path.join(directory_path, file)
        # Read the CSV file into a DataFrame
        df = pd.read_csv(file_path)
        # Store the DataFrame in the dictionary with the filename (without extension) as the key
        dataframes[file[:-4]] = df

        # Optionally, print the first few rows of each DataFrame
        print(f"First few rows of {file}:")
        print(df.head())
        print("\n")

import os

# Path to the directory containing CSV files
directory_path = '/content/drive/MyDrive/2_reviews_per_movie_raw'

# List all files in the directory
files = os.listdir(directory_path)
print("Files in directory:", files)

# Step 3: Read All CSV Files
import pandas as pd

# Initialize an empty dictionary to hold DataFrames
dataframes = {}

# Iterate over each file in the directory
for file in files:
    # Check if the file is a CSV
    if file.endswith('.csv'):
        # Construct full file path
        file_path = os.path.join(directory_path, file)
        # Read the CSV file into a DataFrame
        df = pd.read_csv(file_path)
        # Store the DataFrame in the dictionary with the filename (without extension) as the key
        dataframes[file[:-4]] = df

        # Optionally, print the first few rows of each DataFrame
        print(f"First few rows of {file}:")
        print(df.head())
        print("\n")

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/2_reviews_per_movie_raw/10 Cloverfield Lane 2016.csv')
df.head()

import dask.dataframe as dd

df1 = dd.read_csv("/content/drive/MyDrive/2_reviews_per_movie_raw//B*.csv")
df1.head()

df2 = df1[["review", "rating"]]
df2.head()

df3 = df2[df2.rating != "Null"]
df3.head()

df3.rating = df3.rating.astype("int")
df3.head()

df3.rating = (df3.rating > 5).astype("int")
df3.head()

df3.to_csv("files.csv", index=False)

"""Run TFX components interactively
In the cells that follow, we create TFX components one-by-one, run each of them, and visualize their output artifacts.

### ExampleGen

The `ExampleGen` component is usually at the start of a TFX pipeline. It will:

1.   **Split** data( placed in _data_root ) into training and evaluation sets (by default, 2/3 training + 1/3 eval)
2.   Convert data into the `tf.Example` format
3.   **Copy splits** into the `_tfx_root` directory for other components to access

`ExampleGen` takes as input the path to your data source. In our case, this is the `_data_root` path that contains the downloaded CSV.

Note: In this notebook, we can instantiate components one-by-one and run them with `InteractiveContext.run()`. By contrast, in a production setting, we would specify all the components upfront in a `Pipeline` to pass to the orchestrator (see the [Building a TFX Pipeline Guide](../tfx/guide/build_tfx_pipeline)).
"""

# Importing necessary TFX components and modules
from tfx.components import CsvExampleGen
from tfx.proto import example_gen_pb2
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
# Setting up an interactive TFX context
# This allows running TFX components interactively in environments like Jupyter notebooks.
context = InteractiveContext( pipeline_root='pipeline')

import tensorflow as tf
import os
import pprint
pp = pprint.PrettyPrinter()

# Configuring the output splits for the ExampleGen component
# This specifies how to split the data into training and evaluation sets.
output = example_gen_pb2.Output(
    split_config = example_gen_pb2.SplitConfig(splits=[
        example_gen_pb2.SplitConfig.Split(name="train", hash_buckets=8),
        example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2)
    ])
)

example_gen = CsvExampleGen(input_base='files.csv', output_config=output)
context.run(example_gen)

"""Let's examine the output artifacts of `ExampleGen`. This component produces two artifacts, training examples and evaluation examples:"""

train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')

tfrecord_filenames = [os.path.join(train_uri, name) for name in os.listdir(train_uri)]

dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type='GZIP')

for tfrecord in dataset.take(2):
    serialized_example = tfrecord.numpy()
    example = tf.train.Example()
    example.ParseFromString(serialized_example)
    pp.pprint(example)

"""### StatisticsGen
The `StatisticsGen` component **computes statistics** over your dataset for data analysis, as well as for use in downstream components. It uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.

`StatisticsGen` takes as input the dataset we just ingested using `ExampleGen`.
"""

from tfx.components import StatisticsGen
statistics_gen = StatisticsGen(
    examples = example_gen.outputs['examples']
)
context.run(statistics_gen)

"""After `StatisticsGen` finishes running, we can visualize the outputted statistics - **TFDV**. Try playing with the different plots!"""

context.show(statistics_gen.outputs['statistics'])

"""### SchemaGen

The `SchemaGen` component generates a schema based on your data statistics( outputs of StatisticsGen ). (A schema defines the expected bounds, types, and properties of the features in your dataset.) It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.

Note: The generated schema is best-effort and only tries to infer basic properties of the data. It is expected that you review and modify it as needed.

`SchemaGen` will take as input the statistics that we generated with `StatisticsGen`, looking at the training split by default.
"""

from tfx.components import SchemaGen
schema_gen =  SchemaGen(
    statistics=statistics_gen.outputs['statistics']
)
context.run(schema_gen)
context.show(schema_gen.outputs['schema'])

"""### ExampleValidator
The `ExampleValidator` component detects anomalies in your data, based on the expectations defined by the schema. It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.

`ExampleValidator` will take as input the statistics from `StatisticsGen`, and the schema from `SchemaGen`.
"""

from tfx.components import ExampleValidator
example_validator = ExampleValidator(
    statistics=statistics_gen.outputs['statistics'],
    schema=schema_gen.outputs['schema']
)

context.run(example_validator)
context.show(example_validator.outputs['anomalies'])

"""In the anomalies table, we can see that there are no anomalies. This is what we'd expect, since this the first dataset that we've analyzed and the schema is tailored to it. You should review this schema -- anything unexpected means an anomaly in the data. Once reviewed, the schema can be used to guard future data, and anomalies produced here can be used to debug model performance, understand how your data evolves over time, and identify data errors."""

_transform_module_file = '_transform.py'

"""### Transform
We can use TFT here but, specifically I am using other options like constants calculated via Pandas / Numpy etc. These all will be stored on a **constants_trainer.py** file and then used in trainer.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {_transform_module_file}
# 
# import tensorflow as tf
# import tensorflow_transform as tft
# 
# stopwords = ["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your",
#              "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she",
#              "her", "hers", "herself", "it", "its", "itself", "they", "them", "their",
#              "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these",
#              "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had",
#              "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because",
#              "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into",
#              "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on",
#              "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how",
#              "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only",
#              "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
# 
# _LABEL_KEY = 'rating'
# 
# # Renaming transformed features
# def _transformed_name(key):
#     return key + '_xf'
# 
# # Define the transformations
# def preprocessing_fn(inputs):
# 
#     outputs = {}
# 
#     outputs[_transformed_name('review')] = tf.strings.lower(inputs['review'])
#     outputs[_transformed_name('review')] = tf.strings.regex_replace(outputs[_transformed_name('review')], r"(?:<br />)", " ")
#     outputs[_transformed_name('review')] = tf.strings.regex_replace(outputs[_transformed_name('review')], "n\'t", " not ")
#     outputs[_transformed_name('review')] = tf.strings.regex_replace(outputs[_transformed_name('review')], r"(?:\'ll |\'re |\'d |\'ve)", " ")
#     outputs[_transformed_name('review')] = tf.strings.regex_replace(outputs[_transformed_name('review')], r"\W+", " ")
#     outputs[_transformed_name('review')] = tf.strings.regex_replace(outputs[_transformed_name('review')], r"\d+", " ")
#     outputs[_transformed_name('review')] = tf.strings.regex_replace(outputs[_transformed_name('review')], r"\b[a-zA-Z]\b", " ")
#     outputs[_transformed_name('review')] = tf.strings.regex_replace(outputs[_transformed_name('review')], r'\b(' + r'|'.join(stopwords) + r')\b\s*', " ")
# 
#     outputs[_transformed_name(_LABEL_KEY)] = tf.cast(inputs[_LABEL_KEY], tf.int64)
# 
#     return outputs

# Run the transform component
from tfx.components import Transform
transform  = Transform(
    examples=example_gen.outputs['examples'],
    schema= schema_gen.outputs['schema'],
    module_file=_transform_module_file
)

context.run(transform)

train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')

tfrecord_filenames = [os.path.join(train_uri, name) for name in os.listdir(train_uri)]

dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type='GZIP')

for tfrecord in dataset.take(2):
    serialized_example = tfrecord.numpy()
    example = tf.train.Example()
    example.ParseFromString(serialized_example)
    pp.pprint(example)

# Declare a trainer module file
_trainer_module_file = '_trainer.py'

"""### Trainer
The `Trainer` component will train a model that you define in TensorFlow. Default Trainer support Estimator API, to use Keras API, you need to specify [Generic Trainer](https://github.com/tensorflow/community/blob/master/rfcs/20200117-tfx-generic-trainer.md) by setup `custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor)` in Trainer's contructor.

`Trainer` takes as input the schema from `SchemaGen`, the transformed data and graph from `Transform`, training parameters, as well as a module that contains user-defined model code.

Will generate two files:
- **inputfn_trainer.py** *Data-Feeder to model
- **model_trainer.py** *Trainer module
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {_trainer_module_file}
# 
# import tensorflow as tf
# import tensorflow_transform as tft
# from tensorflow.keras import layers
# import tensorflow_hub as hub
# from tfx.components.trainer.fn_args_utils import FnArgs
# 
# _LABEL_KEY = 'rating'
# _FEATURE = 'review'
# 
# def _transformed_name(key):
#     return key + '_xf'
# 
# def _gzip_reader_fn(filenames):
# 
#     '''Loads compressed data'''
#     return tf.data.TFRecordDataset(filenames, compression_type='GZIP')
# 
# 
# def _input_fn(file_pattern,
#              tf_transform_output,
#              num_epochs,
#              batch_size=64)->tf.data.Dataset:
# 
#     # Get post_transform feature spec
#     transform_feature_spec = (
#         tf_transform_output.transformed_feature_spec().copy())
# 
#     # create batches of data
#     dataset = tf.data.experimental.make_batched_features_dataset(
#         file_pattern=file_pattern,
#         batch_size=batch_size,
#         features=transform_feature_spec,
#         reader=_gzip_reader_fn,
#         num_epochs=num_epochs,
#         label_key = _transformed_name(_LABEL_KEY))
#     return dataset
# 
# 
# 
# embed = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder/4")
# def model_builder():
# 
#     rate = 0.2
# 
#     inputs = tf.keras.Input(shape=(1,), name=_transformed_name('review'), dtype=tf.string)
#     reshaped_narrative = tf.reshape(inputs, [-1])
#     x = embed(reshaped_narrative)
#     x = tf.keras.layers.Reshape((1,512), input_shape=(1,512))(x)
#     x = layers.Dense(64, activation='elu', kernel_initializer='glorot_uniform')(x)
# 
#     attn_output = layers.MultiHeadAttention(num_heads=2, key_dim=64)(x, x, x)
#     attn_output = layers.Dropout(rate)(attn_output)
# 
#     out1 = layers.LayerNormalization(epsilon=1e-7)(x + attn_output)
#     ffn_output = layers.Dense(64, activation="elu", kernel_initializer="glorot_uniform")(out1)
#     ffn_output = layers.Dense(64, kernel_initializer='glorot_uniform')(ffn_output)
#     ffn_output = layers.Dropout(rate)(ffn_output)
# 
#     x = layers.LayerNormalization(epsilon=1e-7)(out1 + ffn_output)
#     x = layers.GlobalAveragePooling1D()(x)
#     x = layers.Dropout(rate)(x)
#     x = layers.Dense(32, activation="elu", kernel_initializer="glorot_uniform")(x)
#     x = layers.Dropout(rate)(x)
#     outputs = layers.Dense(1, activation='sigmoid')(x)
# 
# 
#     model = tf.keras.Model(inputs=inputs, outputs = outputs)
# 
#     model.compile(
#         loss = 'binary_crossentropy',
#         optimizer=tf.keras.optimizers.Adam(0.01),
#         metrics=[tf.keras.metrics.BinaryAccuracy()]
# 
#     )
# 
#     # print(model)
#     model.summary()
#     return model
# 
# 
# def _get_serve_tf_examples_fn(model, tf_transform_output):
# 
#     model.tft_layer = tf_transform_output.transform_features_layer()
# 
#     @tf.function
#     def serve_tf_examples_fn(serialized_tf_examples):
# 
#         feature_spec = tf_transform_output.raw_feature_spec()
# 
#         feature_spec.pop("rating")
# 
#         parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
# 
#         transformed_features = model.tft_layer(parsed_features)
# 
#         # get predictions using the transformed features
#         return model(transformed_features)
# 
#     return serve_tf_examples_fn
# 
# def run_fn(fn_args: FnArgs) -> None:
# 
#     tensorboard_callback = tf.keras.callbacks.TensorBoard(
#         log_dir = fn_args.model_run_dir, update_freq='batch'
#     )
# 
#     es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=10)
#     mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)
# 
# 
#     # Load the transform output
#     tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
# 
#     # Create batches of data
#     train_set = _input_fn(fn_args.train_files, tf_transform_output, 10)
#     val_set = _input_fn(fn_args.eval_files, tf_transform_output, 10)
# 
# 
#     # Build the model
#     model = model_builder()
# 
# 
#     # Train the model
#     model.fit(x = train_set,
#              validation_data = val_set,
#              callbacks = [tensorboard_callback, es, mc],
#                steps_per_epoch = 1000,
#              validation_steps= 1000,
#              epochs=1)
#     signatures = {
#         'serving_default':
#         _get_serve_tf_examples_fn(model,
#                                  tf_transform_output).get_concrete_function(
#                                     tf.TensorSpec(
#                                     shape=[None],
#                                     dtype=tf.string,
#                                     name='examples'))
#     }
#     model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)

from tfx.components import Trainer
from tfx.proto import trainer_pb2

trainer  = Trainer(
    module_file=_trainer_module_file,
    examples = transform.outputs['transformed_examples'],
    transform_graph=transform.outputs['transform_graph'],
    schema=schema_gen.outputs['schema'],
    train_args=trainer_pb2.TrainArgs(splits=['train']),
    eval_args=trainer_pb2.EvalArgs(splits=['eval'])
)

context.run(trainer)

"""#### Analyze Training with TensorBoard
Take a peek at the trainer artifact. It points to a directory containing the model subdirectories.
"""

# Commented out IPython magic to ensure Python compatibility.
model_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri

# %load_ext tensorboard
# %tensorboard --logdir {model_run_artifact_dir}

"""# **Hyperparameter Tuning**"""

# Declare a tuner module file
_tuner_module_file = '_tuner.py'

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {_tuner_module_file}
# import os
# import tensorflow as tf
# import tensorflow_transform as tft
# import keras_tuner as kt
# from tensorflow.keras import layers
# from tfx.components.trainer.fn_args_utils import FnArgs
# from keras_tuner.engine import base_tuner
# from typing import NamedTuple, Dict, Text, Any
# 
# _LABEL_KEY = 'rating'
# _FEATURE_KEY = 'review'
# 
# def transformed_name(key):
#     """Renaming transformed features"""
#     return key + "_xf"
# 
# def gzip_reader_fn(filenames):
#     """Loads compressed data"""
#     return tf.data.TFRecordDataset(filenames, compression_type='GZIP')
# 
# def input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=64) -> tf.data.Dataset:
#     """Get post_tranform feature & create batches of data"""
# 
#     # Get post_transform feature spec
#     transform_feature_spec = (
#         tf_transform_output.transformed_feature_spec().copy()
#     )
# 
#     # create batches of data
#     dataset = tf.data.experimental.make_batched_features_dataset(
#         file_pattern = file_pattern,
#         batch_size   = batch_size,
#         features     = transform_feature_spec,
#         reader       = gzip_reader_fn,
#         num_epochs   = num_epochs,
#         label_key    = transformed_name(_LABEL_KEY)
#     )
# 
#     return dataset
# 
# # Vocabulary size and number of words in a sequence.
# VOCAB_SIZE      = 10000
# SEQUENCE_LENGTH = 100
# 
# vectorize_layer = layers.TextVectorization(
#     standardize            = 'lower_and_strip_punctuation',
#     max_tokens             = VOCAB_SIZE,
#     output_mode            = 'int',
#     output_sequence_length = SEQUENCE_LENGTH
# )
# 
# def model_builder(hp):
#     """Build keras tuner model"""
#     embedding_dim = hp.Int('embedding_dim', min_value=16, max_value=128, step=16)
#     lstm_units    = hp.Int('lstm_units', min_value=16, max_value=128, step=16)
#     num_layers    = hp.Choice('num_layers', values=[1, 2, 3])
#     dense_units   = hp.Int('dense_units', min_value=16, max_value=128, step=16)
#     dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)
#     learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
# 
#     inputs = tf.keras.Input(shape=(1,), name=transformed_name(_FEATURE_KEY), dtype=tf.string)
# 
#     reshaped_narrative = tf.reshape(inputs, [-1])
#     x = vectorize_layer(reshaped_narrative)
#     x = layers.Embedding(VOCAB_SIZE, embedding_dim, name='embedding')(x)
#     x = layers.Bidirectional(layers.LSTM(lstm_units))(x)
#     for _ in range(num_layers):
#         x = layers.Dense(dense_units, activation='relu')(x)
#     x = layers.Dropout(dropout_rate)(x)
#     outputs = layers.Dense(1, activation='sigmoid')(x)
# 
#     model = tf.keras.Model(inputs = inputs, outputs = outputs)
#     model.compile(
#         loss      = tf.keras.losses.BinaryCrossentropy(from_logits=True),
#         optimizer = tf.keras.optimizers.Adam(learning_rate),
#         metrics   = [tf.keras.metrics.BinaryAccuracy()]
#     )
# 
#     model.summary()
#     return model
# 
# TunerFnResult = NamedTuple('TunerFnResult', [
#     ('tuner', base_tuner.BaseTuner),
#     ('fit_kwargs', Dict[Text, Any]),
# ])
# 
# early_stop_callback = tf.keras.callbacks.EarlyStopping(
#     monitor  = 'val_binary_accuracy',
#     mode     = 'max',
#     verbose  = 1,
#     patience = 10
# )
# 
# def tuner_fn(fn_args: FnArgs) -> None:
#     # Load the transform output
#     tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
# 
#     # Create batches of data
#     train_set = input_fn(fn_args.train_files[0], tf_transform_output, 10)
#     val_set   = input_fn(fn_args.eval_files[0],  tf_transform_output, 10)
# 
#     vectorize_layer.adapt(
#         [j[0].numpy()[0] for j in [
#             i[0][transformed_name(_FEATURE_KEY)]
#                 for i in list(train_set)
#         ]]
#     )
# 
#     # Build the model tuner
#     model_tuner = kt.RandomSearch(
#         hypermodel   = lambda hp: model_builder(hp),
#         objective    = kt.Objective('val_binary_accuracy', direction='max'),
#         max_trials   = 3,
#         executions_per_trial = 1,
#         directory    = fn_args.working_dir,
#         project_name = 'imdb_kt'
#     )
# 
#     return TunerFnResult(
#         tuner      = model_tuner,
#         fit_kwargs = {
#             'callbacks'        : [early_stop_callback],
#             'x'                : train_set,
#             'validation_data'  : val_set,
#             'steps_per_epoch'  : fn_args.train_steps,
#             'validation_steps' : fn_args.eval_steps
#         }
#     )
# 
#

from tfx.components import Tuner
tuner = Tuner(
    module_file     = os.path.abspath(_tuner_module_file),
    examples        = transform.outputs['transformed_examples'],
    transform_graph = transform.outputs['transform_graph'],
    schema          = schema_gen.outputs['schema'],
    train_args      = trainer_pb2.TrainArgs(splits=['train']),
    eval_args       = trainer_pb2.EvalArgs(splits=['eval'])
)

context.run(tuner)

"""# **Model Resolver**"""

from tfx.dsl.components.common.resolver import Resolver
from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy
from tfx.types import Channel
from tfx.types.standard_artifacts import Model, ModelBlessing

model_resolver = Resolver(
    strategy_class= LatestBlessedModelStrategy,
    model = Channel(type=Model),
    model_blessing = Channel(type=ModelBlessing)
).with_id('Latest_blessed_model_resolver')

context.run(model_resolver)

"""# **Evaluator**"""

import tensorflow_model_analysis as tfma

eval_config = tfma.EvalConfig(
    model_specs=[tfma.ModelSpec(label_key='rating')],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=[
        tfma.MetricsSpec(metrics=[

            tfma.MetricConfig(class_name='ExampleCount'),
            tfma.MetricConfig(class_name='AUC'),
            tfma.MetricConfig(class_name='FalsePositives'),
            tfma.MetricConfig(class_name='TruePositives'),
            tfma.MetricConfig(class_name='FalseNegatives'),
            tfma.MetricConfig(class_name='TrueNegatives'),
            tfma.MetricConfig(class_name='BinaryAccuracy',
                threshold=tfma.MetricThreshold(
                    value_threshold=tfma.GenericValueThreshold(
                        lower_bound={'value':0.5}),
                    change_threshold=tfma.GenericChangeThreshold(
                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                        absolute={'value':0.0001})
                    )
            )
        ])
    ]

)

from tfx.components import Evaluator
evaluator = Evaluator(
    examples=example_gen.outputs['examples'],
    model=trainer.outputs['model'],
    baseline_model=model_resolver.outputs['model'],
    eval_config=eval_config)

context.run(evaluator)

# Visualize the evaluation results
eval_result = evaluator.outputs['evaluation'].get()[0].uri
tfma_result = tfma.load_eval_result(eval_result)
tfma.view.render_slicing_metrics(tfma_result)

tfma.addons.fairness.view.widget_view.render_fairness_indicator(tfma_result)

# Print validation results
eval_result = evaluator.outputs['evaluation'].get()[0].uri
print(tfma.load_validation_result(eval_result))

"""### Pusher
The `Pusher` component is usually at the end of a TFX pipeline. It checks whether a model has passed validation, and if so, exports the model to `_serving_model_dir`.
"""

from tfx.components import Pusher
from tfx.proto import pusher_pb2

pusher = Pusher(
model=trainer.outputs['model'],
model_blessing=evaluator.outputs['blessing'],
push_destination=pusher_pb2.PushDestination(
    filesystem=pusher_pb2.PushDestination.Filesystem(
        base_directory='serving_model_dir'))

)

context.run(pusher)

"""# **Code for Drift Detection for checking if there is any drift in the model**"""

import os
from tfx.components.base import base_component, base_executor, executor_spec
from tfx.types import standard_artifacts, component_spec, channel_utils
import tensorflow_data_validation as tfdv
from scipy.stats import ks_2samp

class DriftDetectionSpec(component_spec.ComponentSpec):
    PARAMETERS = {}
    INPUTS = {
        'anomalies': component_spec.ChannelParameter(type=standard_artifacts.ExampleAnomalies),
    }
    OUTPUTS = {
        'drift_detected': component_spec.ChannelParameter(type=standard_artifacts.String),
    }

# Assuming anomalies_dir is the directory containing the anomalies
anomalies_dir = '/content/pipeline/ExampleValidator/anomalies/4'

# Define paths to check for SchemaDiff.pb
anomalies_files = [
    os.path.join(anomalies_dir, 'Split-train', 'SchemaDiff.pb'),
    os.path.join(anomalies_dir, 'Split-eval', 'SchemaDiff.pb')
]

# Example usage in the DriftDetectionExecutor class
class DriftDetectionExecutor(base_executor.BaseExecutor):
    def Do(self, input_dict, output_dict, exec_properties):
        anomalies_channel = input_dict.get('anomalies', None)

        # Print input_dict for debugging
        print("input_dict:", input_dict)

        # Check if anomalies_channel is valid and contains artifacts
        if anomalies_channel and len(anomalies_channel) > 0:
            anomalies_uri = anomalies_channel[0].uri
            print(f"Anomalies found at: {anomalies_uri}")
        else:
            print("No anomalies found. Proceeding with drift detection on available data.")
            anomalies_uri = None

        # List contents of the anomalies directory
        print("Contents of anomalies directory:")
        for root, dirs, files in os.walk(anomalies_dir):
            for name in files:
                print(os.path.join(root, name))

        # Check if specific files exist and print their paths
        for anomalies_file in anomalies_files:
            if os.path.exists(anomalies_file):
                print(f"File exists: {anomalies_file}")
            else:
                print(f"File does not exist: {anomalies_file}")

        # Iterate over each anomalies file path and perform drift detection
        for anomalies_file in anomalies_files:
            if os.path.exists(anomalies_file):
                reference_stats_path = anomalies_file
                break
        else:
            # If no valid anomalies file found, handle accordingly
            print("No valid SchemaDiff.pb file found in the specified paths.")
            return

        # Load new data statistics if anomalies are found
        if anomalies_uri:
            new_data_stats_path = os.path.join(anomalies_uri, 'Split-eval', 'stats_tfrecord')
            if os.path.exists(new_data_stats_path):
                new_stats = tfdv.load_statistics(new_data_stats_path)
            else:
                print(f"New data statistics path does not exist: {new_data_stats_path}")
                new_stats = None
        else:
            new_stats = None

        # Log reference and new stats for debugging
        print(f"Reference stats path: {reference_stats_path}")
        if new_stats:
            print(f"New stats loaded from: {new_data_stats_path}")
        else:
            print("No new stats loaded.")

        drift_detected = self.detect_statistical_drift(reference_stats_path, new_stats)

        # Save the result
        drift_detected_output_dir = output_dict['drift_detected'][0].uri
        os.makedirs(drift_detected_output_dir, exist_ok=True)
        drift_detected_output_file = os.path.join(drift_detected_output_dir, 'drift_detected.txt')

        with open(drift_detected_output_file, 'w') as f:
            f.write(drift_detected)

        print("Drift detected" if drift_detected == 'true' else "No drift detected")

    def detect_statistical_drift(self, reference_stats_path, new_stats):
        if not new_stats:
            print("No new statistics to compare. Skipping drift detection.")
            return 'false'

        reference_stats = tfdv.load_statistics(reference_stats_path)
        drift_detected = False

        for feature in new_stats.datasets[0].features:
            if feature.type == tfdv.FeatureType.FLOAT:
                ref_values = [x.num for x in reference_stats.datasets[0].features[feature.name].num_stats.histograms[0].buckets]
                new_values = [x.num for x in new_stats.datasets[0].features[feature.name].num_stats.histograms[0].buckets]

                statistic, p_value = ks_2samp(ref_values, new_values)

                # Log feature values and p-value for debugging
                print(f"Feature: {feature.name}, Ref values: {ref_values}, New values: {new_values}, p-value: {p_value}")

                # Set a threshold for drift detection
                if p_value < 0.05:  # Example threshold, adjust as needed
                    print(f"Drift detected in feature: {feature.name}")
                    drift_detected = True

        return 'true' if drift_detected else 'false'

class DriftDetection(base_component.BaseComponent):
    SPEC_CLASS = DriftDetectionSpec
    EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(DriftDetectionExecutor)

    def __init__(self, anomalies):
        spec = DriftDetectionSpec(
            anomalies=anomalies,
            drift_detected=channel_utils.as_channel([standard_artifacts.String()])
        )
        super(DriftDetection, self).__init__(spec=spec)

# Assuming example_validator is already defined and outputs 'anomalies'
example_validator_output = example_validator.outputs['anomalies']
drift_detection = DriftDetection(anomalies=example_validator_output)
context.run(drift_detection)

"""# **If drift detected then it should retrain the model on another dataset to avoid drift in the model**"""

import os
import glob
import pandas as pd
from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
from tfx.proto import example_gen_pb2

# Define datasets and paths dynamically or through configuration
datasets = [
    '/content/drive/MyDrive/Horror.csv',
    '/content/drive/MyDrive/Animation.csv'
]
current_dataset_index = 0

# Assuming anomalies_dir is the directory containing the anomalies
anomalies_dir = '/content/pipeline/ExampleValidator/anomalies'

context = InteractiveContext()

# Function to ensure consistent headers in CSV files
def ensure_consistent_headers(csv_files):
    if not csv_files:
        return  # Handle the case where no CSV files are found

    # Use pandas to read and standardize headers
    reference_df = pd.read_csv(csv_files[0])
    reference_header = reference_df.columns

    for file in csv_files[1:]:
        df = pd.read_csv(file)
        if not df.columns.equals(reference_header):
            print(f"Adjusting headers for {file}...")
            df.columns = reference_header
            df.to_csv(file, index=False)

# Define initial pipeline components for dataset_1
dataset_path = datasets[current_dataset_index]
csv_files = glob.glob(os.path.join(dataset_path, '*.csv'))
ensure_consistent_headers(csv_files)  # Ensure headers are consistent before ExampleGen

# Corrected example_gen initialization
example_gen = CsvExampleGen(
    input_base=os.path.dirname(dataset_path),  # Use the directory containing the CSV
    input_config=example_gen_pb2.Input(splits=[
        example_gen_pb2.Input.Split(name='train', pattern=os.path.basename(dataset_path))  # Use the filename
    ]),
    output_config=example_gen_pb2.Output(
        split_config=example_gen_pb2.SplitConfig(splits=[
            example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2),
        ])
    )
)

statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])
schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])
example_validator = ExampleValidator(
    statistics=statistics_gen.outputs['statistics'],
    schema=schema_gen.outputs['schema']
)

# DriftDetection component placeholder
# Define and implement DriftDetection component as discussed earlier

# Pipeline execution with iterative logic for drift detection and retraining
while current_dataset_index < len(datasets):
    # Run the components for the current dataset
    context.run(example_gen)
    context.run(statistics_gen)
    context.run(schema_gen)
    context.run(example_validator)  # Make sure ExampleValidator is run

    # Print ExampleValidator output artifact URI
    print("ExampleValidator anomalies URI:", example_validator.outputs['anomalies'].get()[0].uri)

    # Perform drift detection
    context.run(drift_detection)
    drift_detected = context.show(drift_detection.outputs['drift_detected'])

    if drift_detected == 'true':
        print(f"Drift detected in {dataset_path}. Retraining on next dataset.")
    else:
        print("No drift detected. Proceeding to the next dataset.")

    # Move to the next dataset
    current_dataset_index += 1
    if current_dataset_index < len(datasets):
        dataset_path = datasets[current_dataset_index]
        csv_files = glob.glob(os.path.join(dataset_path, '*.csv'))
        ensure_consistent_headers(csv_files)  # Ensure headers are consistent before updating ExampleGen
        example_gen.input_base = dataset_path  # Update input_base for ExampleGen

# Finalize the pipeline execution
# context.close()

"""# **Code for printing the metrics of the model**"""

# Commented out IPython magic to ensure Python compatibility.
!pip install tensorflow-model-analysis

import tensorflow_model_analysis as tfma
import os
import json
import tensorflow as tf

# Define the log directory for TensorBoard
log_dir = 'logs/metrics'
file_writer = tf.summary.create_file_writer(log_dir)

# Load and render evaluation metrics
eval_result = evaluator.outputs['evaluation'].get()[0].uri
tfma_result = tfma.load_eval_result(eval_result)
tfma.view.render_slicing_metrics(tfma_result)
tfma.addons.fairness.view.widget_view.render_fairness_indicator(tfma_result)

# Print validation results
print("Validation Results:")
validation_result = tfma.load_validation_result(eval_result)
print(validation_result)

# Calculate and print precision, recall, and F1-score
try:
    slicing_metrics = tfma_result.slicing_metrics[0][1]['']['']
    true_positives = slicing_metrics['true_positives']['doubleValue']
    false_positives = slicing_metrics['false_positives']['doubleValue']
    false_negatives = slicing_metrics['false_negatives']['doubleValue']

    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1-Score: {f1_score}")

    # Log metrics to TensorBoard
    with file_writer.as_default():
        tf.summary.scalar('Precision', precision, step=0)
        tf.summary.scalar('Recall', recall, step=0)
        tf.summary.scalar('F1-Score', f1_score, step=0)

except (KeyError, IndexError, TypeError) as e:
    print(f"Error accessing metrics: {e}")

# Start TensorBoard within Colab
# %reload_ext tensorboard
# %tensorboard --logdir logs/metrics

"""# **Plotting the confusion matrix based on true and false values**"""

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have the following variables from your evaluation
precision = 0.8780225685115529
recall = 0.918364479415484
f1_score = 0.8977405397980908

# Compute confusion matrix values
true_positives = precision * recall  # TP = precision * recall
false_positives = (1 - precision) * recall  # FP = (1 - precision) * recall
false_negatives = (1 - recall) * precision  # FN = (1 - recall) * precision
true_negatives = 1 - (true_positives + false_positives + false_negatives)  # TN = 1 - (TP + FP + FN)

# Create confusion matrix
conf_matrix = np.array([[true_negatives, false_positives],
                        [false_negatives, true_positives]])

# Plot confusion matrix
plt.figure(figsize=(8, 6))
plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()

tick_marks = np.arange(2)
plt.xticks(tick_marks, ['Predicted Negative', 'Predicted Positive'], rotation=45)
plt.yticks(tick_marks, ['Actual Negative', 'Actual Positive'])

thresh = conf_matrix.max() / 2.
for i, j in ((i, j) for i in range(2) for j in range(2)):
    plt.text(j, i, format(conf_matrix[i, j], '.2f'),
             horizontalalignment="center",
             color="white" if conf_matrix[i, j] > thresh else "black")

plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

!pip install -U tf-nightly

import tensorflow_model_analysis as tfma
from tfx import v1 as tfx

# Assuming you have already loaded the evaluation results URI into eval_result_path
eval_result_path = evaluator.outputs['evaluation'].get()[0].uri

# Load the evaluation result using TFMA
eval_result = tfma.load_eval_result(eval_result_path)

# Extract metrics (assuming it's a list)
metrics = eval_result.slicing_metrics

# Iterate over the list and print each metric (or its keys if it's a dictionary)
for metric in metrics:
    if isinstance(metric, dict):
        print(metric.keys())
    else:
        print(metric)

# Access specific metrics (example - adjust based on the structure of your metrics)
# Example if metrics are dictionaries within the list
if metrics and isinstance(metrics[0], dict) and 'your_metric_key' in metrics[0]:
    your_metric = metrics[0]['your_metric_key']
    print(your_metric)

!pip install pyngrok

import os
from pyngrok import ngrok

# Set your ngrok authtoken
ngrok.set_auth_token('2j6iWD7kcXeiZqs9gaN4d5hirO0_5qTu689XyxMyVWJKffzSt')

# Function to disconnect all existing ngrok tunnels
def disconnect_all_ngrok_tunnels():
    tunnels = ngrok.get_tunnels()
    for tunnel in tunnels:
        ngrok.disconnect(tunnel.public_url)
    print("Disconnected all existing ngrok tunnels")

# Disconnect existing tunnels
disconnect_all_ngrok_tunnels()

best_hyperparameters_file = '/content/pipeline/Tuner/best_hyperparameters/7/best_hyperparameters.txt'

try:
    with open(best_hyperparameters_file, 'r') as f:
        best_hyperparameters = f.read()
        print("Best Hyperparameters:")
        print(best_hyperparameters)
except FileNotFoundError:
    print(f"Could not find the best hyperparameters file: {best_hyperparameters_file}")

import json

# Path to your best hyperparameters file
best_hyperparameters_file = '/content/pipeline/Tuner/best_hyperparameters/7/best_hyperparameters.txt'

# Load the JSON content from the file
with open(best_hyperparameters_file, 'r') as f:
    best_hyperparameters = json.load(f)

# Extract hyperparameter values
embedding_dim = best_hyperparameters['values']['embedding_dim']
lstm_units = best_hyperparameters['values']['lstm_units']
num_layers = best_hyperparameters['values']['num_layers']
dense_units = best_hyperparameters['values']['dense_units']
dropout_rate = best_hyperparameters['values']['dropout_rate']
learning_rate = best_hyperparameters['values']['learning_rate']

# Print the best hyperparameters
print(f'Best Hyperparameters:')
print(f'Embedding Dimension: {embedding_dim}')
print(f'LSTM Units: {lstm_units}')
print(f'Number of Layers: {num_layers}')
print(f'Dense Units: {dense_units}')
print(f'Dropout Rate: {dropout_rate}')
print(f'Learning Rate: {learning_rate}')

import tensorflow as tf
from tensorboard.plugins.hparams import api as hp
import json
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

# Path to your best hyperparameters file
best_hyperparameters_file = '/content/pipeline/Tuner/best_hyperparameters/7/best_hyperparameters.txt'

# Load the JSON content from the file
with open(best_hyperparameters_file, 'r') as f:
    best_hyperparameters = json.load(f)

# Extract hyperparameter values
embedding_dim = best_hyperparameters['values']['embedding_dim']
lstm_units = best_hyperparameters['values']['lstm_units']
num_layers = best_hyperparameters['values']['num_layers']
dense_units = best_hyperparameters['values']['dense_units']
dropout_rate = best_hyperparameters['values']['dropout_rate']
learning_rate = best_hyperparameters['values']['learning_rate']

# Define hyperparameters
HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.IntInterval(embedding_dim, embedding_dim))
HP_LSTM_UNITS = hp.HParam('lstm_units', hp.IntInterval(lstm_units, lstm_units))
HP_NUM_LAYERS = hp.HParam('num_layers', hp.IntInterval(num_layers, num_layers))
HP_DENSE_UNITS = hp.HParam('dense_units', hp.IntInterval(dense_units, dense_units))
HP_DROPOUT_RATE = hp.HParam('dropout_rate', hp.RealInterval(dropout_rate, dropout_rate))
HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(learning_rate, learning_rate))

# Create a log directory for TensorBoard
log_dir = '/content/logs/hparam_tuning'

with tf.summary.create_file_writer(log_dir).as_default():
    hp.hparams_config(
        hparams=[HP_EMBEDDING_DIM, HP_LSTM_UNITS, HP_NUM_LAYERS, HP_DENSE_UNITS, HP_DROPOUT_RATE, HP_LEARNING_RATE],
        metrics=[hp.Metric('accuracy', display_name='Accuracy')],
    )

def build_model(hparams):
    model = Sequential()
    model.add(Embedding(input_dim=10000, output_dim=hparams[HP_EMBEDDING_DIM]))  # Example input_dim
    for _ in range(hparams[HP_NUM_LAYERS] - 1):
        model.add(LSTM(units=hparams[HP_LSTM_UNITS], return_sequences=True))
        model.add(Dropout(rate=hparams[HP_DROPOUT_RATE]))
    model.add(LSTM(units=hparams[HP_LSTM_UNITS]))  # Last LSTM layer without return_sequences
    model.add(Dropout(rate=hparams[HP_DROPOUT_RATE]))
    model.add(Dense(units=hparams[HP_DENSE_UNITS], activation='relu'))
    model.add(Dense(1, activation='sigmoid'))  # Single output for binary classification
    model.compile(optimizer=tf.keras.optimizers.Adam(hparams[HP_LEARNING_RATE]),
                  loss='binary_crossentropy', metrics=['accuracy'])
    return model

def run_experiment(hparams):
    model = build_model(hparams)
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)
    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=200)
    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=200)
    history = model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))
    _, accuracy = model.evaluate(x_test, y_test)
    return accuracy

def log_hyperparameters(hparams, accuracy):
    with tf.summary.create_file_writer(log_dir).as_default():
        hp.hparams(hparams)
        tf.summary.scalar('accuracy', accuracy, step=1)

hparams = {
    HP_EMBEDDING_DIM: embedding_dim,
    HP_LSTM_UNITS: lstm_units,
    HP_NUM_LAYERS: num_layers,
    HP_DENSE_UNITS: dense_units,
    HP_DROPOUT_RATE: dropout_rate,
    HP_LEARNING_RATE: learning_rate,
}

accuracy = run_experiment(hparams)
log_hyperparameters(hparams, accuracy)

import tensorflow as tf
from tensorboard.plugins.hparams import api as hp
import json
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

# ... (rest of your code)

session_num = 0

for embedding_dim in range(HP_EMBEDDING_DIM.domain.min_value, HP_EMBEDDING_DIM.domain.max_value + 1):
    for lstm_units in range(HP_LSTM_UNITS.domain.min_value, HP_LSTM_UNITS.domain.max_value + 1):
        for num_layers in range(HP_NUM_LAYERS.domain.min_value, HP_NUM_LAYERS.domain.max_value + 1):
            for dense_units in range(HP_DENSE_UNITS.domain.min_value, HP_DENSE_UNITS.domain.max_value + 1):
                # For RealInterval, you can directly use min_value and max_value
                for dropout_rate in (HP_DROPOUT_RATE.domain.min_value, HP_DROPOUT_RATE.domain.max_value):
                    for learning_rate in (HP_LEARNING_RATE.domain.min_value, HP_LEARNING_RATE.domain.max_value):
                        hparams = {
                            'embedding_dim': embedding_dim,
                            'lstm_units': lstm_units,
                            'num_layers': num_layers,
                            'dense_units': dense_units,
                            'dropout_rate': dropout_rate,
                            'learning_rate': learning_rate,
                        }
                        run_name = "run-%d" % session_num
                        print('--- Starting trial: %s' % run_name)
                        print(hparams)
                        # Assuming 'run' is defined elsewhere
                        # run('logs/hparam_tuning/' + run_name, hparams)
                        session_num += 1

tensorboard --logdir /content/logs/hparam_tuning

import os
from pyngrok import ngrok

# Set your ngrok authtoken
ngrok.set_auth_token('2j6iWD7kcXeiZqs9gaN4d5hirO0_5qTu689XyxMyVWJKffzSt')

# Function to disconnect all existing ngrok tunnels
def disconnect_all_ngrok_tunnels():
    tunnels = ngrok.get_tunnels()
    for tunnel in tunnels:
        ngrok.disconnect(tunnel.public_url)
    print("Disconnected all existing ngrok tunnels")

# Disconnect existing tunnels
disconnect_all_ngrok_tunnels()

"""# **Connecting to tensorboard and deploying it through Ngork for visualizing the results**"""

import os
from tensorboard import program
from pyngrok import ngrok
import tensorflow as tf
import tensorflow_model_analysis as tfma
import socket

# Set your ngrok authtoken
ngrok.set_auth_token('2j6iWD7kcXeiZqs9gaN4d5hirO0_5qTu689XyxMyVWJKffzSt')

# Define the directories where your logs are stored
model_run_artifact_dir = '/content/pipeline/Trainer/model_run_logs'  # Update with your actual path
hparam_tuning_log_dir = '/content/logs/hparam_tuning'  # Update with your actual path
evaluation_log_dir = 'logs/metrics'

# Create a parent log directory
parent_log_dir = '/content/combined_logs'
os.makedirs(parent_log_dir, exist_ok=True)

# Function to create symbolic links if they don't exist
def create_symlink(target, link_name):
    try:
        os.symlink(target, link_name)
    except FileExistsError:
        pass

# Link all log directories into the parent log directory
create_symlink(model_run_artifact_dir, os.path.join(parent_log_dir, 'model_run_logs'))
create_symlink(hparam_tuning_log_dir, os.path.join(parent_log_dir, 'hparam_tuning_logs'))
create_symlink(evaluation_log_dir, os.path.join(parent_log_dir, 'evaluation_logs'))

# Verify the structure of the log directories
print("Directory structure of parent log directory:")
for root, dirs, files in os.walk(parent_log_dir):
    level = root.replace(parent_log_dir, '').count(os.sep)
    indent = ' ' * 4 * (level)
    print(f"{indent}{os.path.basename(root)}/")
    subindent = ' ' * 4 * (level + 1)
    for f in files:
        print(f"{subindent}{f}")

# Define the log directory for TensorBoard evaluation results
file_writer = tf.summary.create_file_writer(evaluation_log_dir)

# Load and render evaluation metrics
# Replace evaluator with your actual evaluator component
eval_result = evaluator.outputs['evaluation'].get()[0].uri  # Update with your actual evaluator
tfma_result = tfma.load_eval_result(eval_result)
tfma.view.render_slicing_metrics(tfma_result)
tfma.addons.fairness.view.widget_view.render_fairness_indicator(tfma_result)

# Print validation results
print("Validation Results:")
validation_result = tfma.load_validation_result(eval_result)
print(validation_result)

# Calculate and print precision, recall, and F1-score
try:
    slicing_metrics = tfma_result.slicing_metrics[0][1]['']['']
    true_positives = slicing_metrics['true_positives']['doubleValue']
    false_positives = slicing_metrics['false_positives']['doubleValue']
    false_negatives = slicing_metrics['false_negatives']['doubleValue']

    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1-Score: {f1_score}")

    # Log metrics to TensorBoard
    with file_writer.as_default():
        tf.summary.scalar('Precision', precision, step=0)
        tf.summary.scalar('Recall', recall, step=0)
        tf.summary.scalar('F1-Score', f1_score, step=0)

except (KeyError, IndexError, TypeError) as e:
    print(f"Error accessing metrics: {e}")

# Function to find an available port
def find_available_port(start_port=6006, max_attempts=100):
    port = start_port
    while port < start_port + max_attempts:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            if s.connect_ex(('localhost', port)) != 0:
                return port
        port += 1
    raise RuntimeError('No available ports found')

# Find an available port for TensorBoard
available_port = find_available_port()

# Initialize and start TensorBoard for the combined logs
tb = program.TensorBoard()
tb.configure(argv=[None, '--logdir', parent_log_dir, '--port', str(available_port)])
url = tb.launch()

# Start ngrok to tunnel TensorBoard using the available port
public_url = ngrok.connect(addr=str(available_port), proto='http')
print(f'Combined TensorBoard URL: {public_url}')

# Commented out IPython magic to ensure Python compatibility.
import os
from tensorboard import notebook
from pyngrok import ngrok

# Set your ngrok authtoken
ngrok.set_auth_token('2jQYOsqjaVUgghBgIqNrt47S71y_WJyvU3w5VtoBfma1vqs7')

# Define the directory where your model's logs are stored
model_run_artifact_dir = '/content/pipeline/Trainer/model_run_logs'  # Update with your actual path

# Start TensorBoard
# %load_ext tensorboard
# %tensorboard --logdir {model_run_artifact_dir} --port 6011 &

# Start ngrok to tunnel TensorBoard port 6011 using HTTP protocol
public_url = ngrok.connect(addr='6011', proto='http')
print(f'TensorBoard URL: {public_url}')

!zip -r pipeline.zip pipeline/
!zip -r logs.zip logs/
!zip -r serving_model_dir.zip serving_model_dir/
!zip -r files.csv.zip files.csv/
!pip freeze > requirements.txt

!zip -r combined_logs.zip combined_logs/

!cp /content/combined_logs.zip /content/drive/MyDrive